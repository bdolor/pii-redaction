{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Conversion\n",
    "## ECHR JSON files to SpaCy JSON files\n",
    "modifications: de-duplicates documents, re-names some TAB entities to equivalent SpaCy entities, removes entities already covered by the base model\n",
    "\n",
    "modified from original: Brad Payne\n",
    "original file: https://github.com/NorskRegnesentral/text-anonymization-benchmark/blob/master/longformer_experiments/data_manipulation.py\n",
    "original author: Norsk Regnesentral\n",
    "license: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Spacy Entities:**\n",
    "\n",
    "PERSON:      People, including fictional.<br>\n",
    "NORP:        Nationalities or religious or political groups.<br>\n",
    "FAC:         Buildings, airports, highways, bridges, etc.<br>\n",
    "ORG:         Companies, agencies, institutions, etc.<br>\n",
    "GPE:         Countries, cities, states.<br>\n",
    "LOC:         Non-GPE locations, mountain ranges, bodies of water.<br>\n",
    "PRODUCT:     Objects, vehicles, foods, etc. (Not services.)<br>\n",
    "EVENT:       Named hurricanes, battles, wars, sports events, etc.<br>\n",
    "WORK_OF_ART: Titles of books, songs, etc.<br>\n",
    "LAW:         Named documents made into laws.<br>\n",
    "LANGUAGE:    Any named language.<br>\n",
    "DATE:        Absolute or relative dates or periods.<br>\n",
    "TIME:        Times smaller than a day.<br>\n",
    "PERCENT:     Percentage, including ”%“.<br>\n",
    "MONEY:       Monetary values, including unit.<br>\n",
    "QUANTITY:    Measurements, as of weight or distance.<br>\n",
    "ORDINAL:     “first”, “second”, etc.<br>\n",
    "CARDINAL:    Numerals that do not fall under another type.<br>\n",
    "\n",
    "**Text Anonymization Benchmark Entities:**\n",
    "\n",
    "PERSON:\t    Names of people, including nicknames/aliases, usernames and initials<br>\n",
    "CODE:\t    Numbers and codes that identify something, such as SSN, phone number, passport number, license plate<br>\n",
    "LOC:\t    Places and locations, such as: Cities, areas, countries, etc. Addresses Named infrastructures (bus stops, bridges, etc.)<br>\n",
    "ORG:\t    Names of organisations, such as: public and private companies schools, universities, public institutions, prisons, healthcare institutions non-governmental organisations, churches, etc.<br>\n",
    "DEM:\t    Demographic attribute of a person, such as: Native language, descent, heritage, ethnicity Job titles, ranks, education Physical descriptions, diagnosis, birthmarks, ages<br>\n",
    "DATETIME:\tDescription of a specific date (e.g. October 3, 2018), time (e.g. 9:48 AM) or duration (e.g. 18 years).<br>\n",
    "QUANTITY:\tDescription of a meaningful quantity, e.g. percentages or monetary values.<br>\n",
    "MISC:\t    Every other type of information that describes an individual and that does not belong to the categories above<br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-26T20:50:33.841244Z",
     "start_time": "2023-07-26T20:50:32.081292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR 1112\n",
      "TR2 1653\n",
      "TE 555\n",
      "TR-dd 1141\n",
      "TE-dd 127\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "translator = {\n",
    "    \"LOC\": \"GPE\",\n",
    "    \"DATETIME\": \"DATE\",\n",
    "}\n",
    "\n",
    "def update_entity_types(swap:str, entity_mapping:Dict[str,str]):\n",
    "    \"\"\"Replace entity types using a translator dictionary.\"\"\"\n",
    "    if swap in entity_mapping.keys():\n",
    "        return entity_mapping[swap]\n",
    "    else:\n",
    "        return swap\n",
    "\n",
    "def de_dupe(duplicated_list:list):\n",
    "    deduplicated_list = list()\n",
    "    for item in duplicated_list:\n",
    "        if item not in deduplicated_list:\n",
    "            deduplicated_list.append(item)\n",
    "    return deduplicated_list\n",
    "\n",
    "with open('../../data/annotated/echr_train.json', \"r\", encoding=\"utf-8\") as f1, open('../../data/annotated/echr_dev.json', \"r\", encoding=\"utf-8\") as f2, open('../../data/annotated/echr_test.json', \"r\", encoding=\"utf-8\") as f3:\n",
    "    train = json.load(f1)\n",
    "    dev = json.load(f2)\n",
    "    test = json.load(f3)\n",
    "    training_raw = []\n",
    "    dev_raw = []\n",
    "    test_raw = []\n",
    "\n",
    "    # the following entities overlap with entities in the pretrained Spacy model\n",
    "    overlap = ['QUANTITY', 'MISC', 'CODE']\n",
    "    for ann_data in train:\n",
    "        dct = {}\n",
    "        for annotator in ann_data['annotations']:\n",
    "            dct['full_text'] = ann_data['text']\n",
    "            annotations = []\n",
    "            for annotation in ann_data['annotations'][annotator]['entity_mentions']:\n",
    "                span = {\n",
    "                    'entity_type': update_entity_types(annotation['entity_type'], translator),\n",
    "                    'entity_value': annotation['span_text'],\n",
    "                    'start_position': annotation['start_offset'],\n",
    "                    'end_position': annotation['end_offset']\n",
    "                }\n",
    "                if span['entity_type'] not in overlap:\n",
    "                    annotations.append(span)\n",
    "            dct['spans'] = annotations\n",
    "            training_raw.append(dct)\n",
    "    print(\"Number of Training files: \", len(training_raw))\n",
    "    # merging 'dev' into 'training dataset' is intentional, as Spacy doesn't require a dev dataset\n",
    "    for ann_data in dev:\n",
    "        dct = {}\n",
    "        for annotator in ann_data['annotations']:\n",
    "            dct['full_text'] = ann_data['text']\n",
    "            annotations = []\n",
    "            for annotation in ann_data['annotations'][annotator]['entity_mentions']:\n",
    "                span= {\n",
    "                    'entity_type': update_entity_types(annotation['entity_type'], translator),\n",
    "                    'entity_value': annotation['span_text'],\n",
    "                    'start_position': annotation['start_offset'],\n",
    "                    'end_position': annotation['end_offset']\n",
    "                }\n",
    "                if span['entity_type'] not in overlap:\n",
    "                    annotations.append(span)\n",
    "            dct['spans'] = annotations\n",
    "            training_raw.append(dct)\n",
    "    print(\"Number of Dev files appended to Training files: \", len(training_raw))\n",
    "    for ann_data in test:\n",
    "        dct = {}\n",
    "        for annotator in ann_data['annotations']:\n",
    "            dct['full_text'] = ann_data['text']\n",
    "            annotations = []\n",
    "            for annotation in ann_data['annotations'][annotator]['entity_mentions']:\n",
    "                span= {\n",
    "                    'entity_type': update_entity_types(annotation['entity_type'], translator),\n",
    "                    'entity_value': annotation['span_text'],\n",
    "                    'start_position': annotation['start_offset'],\n",
    "                    'end_position': annotation['end_offset']\n",
    "                }\n",
    "                if span['entity_type'] not in overlap:\n",
    "                    annotations.append(span)\n",
    "            dct['spans'] = annotations\n",
    "            test_raw.append(dct)\n",
    "    print(\"Number of Test files: \", len(test_raw))\n",
    "de_duped_training = de_dupe(training_raw)\n",
    "de_duped_test = de_dupe(test_raw)\n",
    "print(\"Number of Training files after de-duplication: \", len(de_duped_training))\n",
    "print(\"Number of Test files after de-duplication: \", len(de_duped_test))\n",
    "with open('../../data/annotated/train_data.json', 'w', encoding='utf-8') as f1:\n",
    "    json.dump(de_duped_training, f1, ensure_ascii=False, indent=4)\n",
    "with open('../../data/annotated/test_data.json', 'w', encoding='utf-8') as f3:\n",
    "    json.dump(de_duped_test, f3, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
