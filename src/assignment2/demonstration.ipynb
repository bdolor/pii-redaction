{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /Users/bpayne/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package propbank to /Users/bpayne/nltk_data...\n",
      "[nltk_data]   Package propbank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.examples import sentences\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# For Text Classification\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "\n",
    "# Load the English language model in Spacy\n",
    "# run `python -m spacy download en_core_web_sm` on the command line if you receive an error message\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Semantic Parsing\n",
    "nltk.download('treebank')\n",
    "nltk.download('propbank')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:14:12.384034Z",
     "start_time": "2023-06-20T22:14:11.548437Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Syntactic Parsing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "# Syntactic Parsing (NLTK implementation)\n",
    "sentence = \"The dog saw the man in the park\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:14:12.397945Z",
     "start_time": "2023-06-20T22:14:12.380743Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Syntactic categories are used to reflect grammatical structures within a sentence.\n",
    "\n",
    "i. S = sentence\n",
    "ii. NP = Noun phrase\n",
    "iii. VP = Verb phrase\n",
    "iv. PP = Prepositional Phrase\n",
    "v. Det = Determiner\n",
    "vi.\tN = Noun\n",
    "vii. V = Verb\n",
    "viii. P = Preposition\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 S                          \n",
      "      ___________|_______                    \n",
      "     |                   VP                 \n",
      "     |            _______|_______            \n",
      "     |           VP              PP         \n",
      "     |        ___|___         ___|___        \n",
      "     NP      |       NP      |       NP     \n",
      "  ___|___    |    ___|___    |    ___|___    \n",
      "Det      N   V  Det      N   P  Det      N  \n",
      " |       |   |   |       |   |   |       |   \n",
      "The     dog saw the     man  in the     park\n",
      "\n",
      "                 S                          \n",
      "      ___________|_______                    \n",
      "     |                   VP                 \n",
      "     |        ___________|___                \n",
      "     |       |               NP             \n",
      "     |       |        _______|___            \n",
      "     |       |       |           PP         \n",
      "     |       |       |        ___|___        \n",
      "     NP      |       NP      |       NP     \n",
      "  ___|___    |    ___|___    |    ___|___    \n",
      "Det      N   V  Det      N   P  Det      N  \n",
      " |       |   |   |       |   |   |       |   \n",
      "The     dog saw the     man  in the     park\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A sentence for parsing (with known ambiguity)\n",
    "# \"The dog saw the man in the park\"\n",
    "\n",
    "# Define a grammar in Chomsky Normal Form\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | NP PP\n",
    "    VP -> V NP | VP PP\n",
    "    PP -> P NP\n",
    "    Det -> 'the' | 'The'\n",
    "    N -> 'dog' | 'man' | 'park'\n",
    "    V -> 'saw'\n",
    "    P -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "# Create a parser using the grammar\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "# Tokenize the sentence\n",
    "t = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Parse the sentence\n",
    "for tree in parser.parse(t):\n",
    "    tree.pretty_print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:14:12.448918Z",
     "start_time": "2023-06-20T22:14:12.407294Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Syntactic Parsing (explained)\n",
    "The two trees above reflect the ambiguity in the sentence\n",
    "\n",
    "Structural ambiguity is demonstrated when sentences can be interpreted in more than one way, for instance, when a PP (Prepositional Phrase) can be attached to either a NP (Noun Phrase) or a VP (Verb Phrase), which changes the meaning of the sentence. An example sentence presented in [1] is “The dog saw the man in the park” which can either mean that the man was in the park when the dog saw him, or the dog was in the park when it saw the man.\n",
    "\n",
    "References:\n",
    "[1] Bird, S., Klein, E., Loper, E. “Natural Language Processing with Python” https://www.nltk.org/book/ (accessed June 19, 2023)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dependency Parsing\n",
    "## Noun Chunks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed:  True \n",
      "\n",
      "Full Text:  San Francisco considers banning sidewalk delivery robots \n",
      "\n",
      "Text Chunk:  San Francisco \n",
      " Root Text:  Francisco \n",
      " Root Dep:  nsubj \n",
      " Root Head Text:  considers\n",
      "Text Chunk:  sidewalk delivery robots \n",
      " Root Text:  robots \n",
      " Root Dep:  dobj \n",
      " Root Head Text:  banning\n"
     ]
    }
   ],
   "source": [
    "# Parse (Spacy Implementation)\n",
    "doc = nlp(sentences[2])\n",
    "\n",
    "# Check whether the object has been parsed\n",
    "print(\"Parsed: \", doc.has_annotation(\"DEP\"), \"\\n\")\n",
    "\n",
    "# Print the sentence\n",
    "print (\"Full Text: \", doc.text, '\\n')\n",
    "\n",
    "# Noun Chunks\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(\"Text Chunk: \", chunk.text, \"\\n\", \"Root Text: \", chunk.root.text, \"\\n\", \"Root Dep: \", chunk.root.dep_, \"\\n\", \"Root Head Text: \",chunk.root.head.text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:14:12.449924Z",
     "start_time": "2023-06-20T22:14:12.418755Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dependency Parsing (continued)\n",
    "## Navigating the Parse Tree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  San \n",
      " Token Dep:  compound \n",
      " Token Head Text:  Francisco \n",
      " Token Head POS:  PROPN \n",
      " []\n",
      "Token:  Francisco \n",
      " Token Dep:  nsubj \n",
      " Token Head Text:  considers \n",
      " Token Head POS:  VERB \n",
      " [San]\n",
      "Token:  considers \n",
      " Token Dep:  ROOT \n",
      " Token Head Text:  considers \n",
      " Token Head POS:  VERB \n",
      " [Francisco, banning]\n",
      "Token:  banning \n",
      " Token Dep:  xcomp \n",
      " Token Head Text:  considers \n",
      " Token Head POS:  VERB \n",
      " [robots]\n",
      "Token:  sidewalk \n",
      " Token Dep:  compound \n",
      " Token Head Text:  delivery \n",
      " Token Head POS:  NOUN \n",
      " []\n",
      "Token:  delivery \n",
      " Token Dep:  compound \n",
      " Token Head Text:  robots \n",
      " Token Head POS:  NOUN \n",
      " [sidewalk]\n",
      "Token:  robots \n",
      " Token Dep:  dobj \n",
      " Token Head Text:  banning \n",
      " Token Head POS:  VERB \n",
      " [delivery]\n"
     ]
    }
   ],
   "source": [
    "# Navigate the Parse Tree\n",
    "for token in doc:\n",
    "        print(\"Token: \", token.text, \"\\n\", \"Token Dep: \", token.dep_, \"\\n\", \"Token Head Text: \", token.head.text, \"\\n\", \"Token Head POS: \", token.head.pos_, \"\\n\", [child for child in token.children])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:14:12.450067Z",
     "start_time": "2023-06-20T22:14:12.443371Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"0118e044953945e19b9040cfa3300649-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">San</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Francisco</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">considers</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">banning</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">sidewalk</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">delivery</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">robots</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n</text>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-0118e044953945e19b9040cfa3300649-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-0118e044953945e19b9040cfa3300649-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-0118e044953945e19b9040cfa3300649-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-0118e044953945e19b9040cfa3300649-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-0118e044953945e19b9040cfa3300649-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-0118e044953945e19b9040cfa3300649-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-0118e044953945e19b9040cfa3300649-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-0118e044953945e19b9040cfa3300649-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M770,179.0 L762,167.0 778,167.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-0118e044953945e19b9040cfa3300649-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-0118e044953945e19b9040cfa3300649-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-0118e044953945e19b9040cfa3300649-0-5\" stroke-width=\"2px\" d=\"M595,177.0 C595,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-0118e044953945e19b9040cfa3300649-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n</g>\n</svg></span>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing dependencies\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='dep')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:14:12.453403Z",
     "start_time": "2023-06-20T22:14:12.446007Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dependency Parsing (explained)\n",
    "\n",
    "When the sentence \"San Francisco considers banning sidewalk delivery robots\" is parsed to display dependency parsing, each line reflects a single token in that sentence with the text, the dependency label and its head (or the token it depends on)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          Resume_str Category\n0           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...       HR\n1           HR SPECIALIST, US HR OPERATIONS      ...       HR\n2           HR DIRECTOR       Summary      Over 2...       HR\n3           HR SPECIALIST       Summary    Dedica...       HR\n4           HR MANAGER         Skill Highlights  ...       HR\n5           HR GENERALIST       Summary     Dedic...       HR\n6           HR MANAGER       Summary    HUMAN RES...       HR\n7           HR MANAGER         Professional Summa...       HR\n8           HR SPECIALIST       Summary    Posses...       HR\n9           HR CLERK       Summary    Translates ...       HR",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Resume_str</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n      <td>HR</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HR SPECIALIST, US HR OPERATIONS      ...</td>\n      <td>HR</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HR DIRECTOR       Summary      Over 2...</td>\n      <td>HR</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HR SPECIALIST       Summary    Dedica...</td>\n      <td>HR</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HR MANAGER         Skill Highlights  ...</td>\n      <td>HR</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>HR GENERALIST       Summary     Dedic...</td>\n      <td>HR</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>HR MANAGER       Summary    HUMAN RES...</td>\n      <td>HR</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>HR MANAGER         Professional Summa...</td>\n      <td>HR</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>HR SPECIALIST       Summary    Posses...</td>\n      <td>HR</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>HR CLERK       Summary    Translates ...</td>\n      <td>HR</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beginning of Text Classification\n",
    "\n",
    "data=pd.read_csv('../../data/resume-kaggle.csv')\n",
    "resume_data=data.drop(columns=[\"ID\",\"Resume_html\"])\n",
    "resume_data.head(10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:15:52.013719Z",
     "start_time": "2023-06-20T22:15:51.175922Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stemming"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer \n",
      "\n",
      "calculate ----> calcul\n",
      "calculator ----> calcul\n",
      "calculated ----> calcul\n",
      "calculating ----> calcul\n",
      "\n",
      " Snowball Stemmer \n",
      "\n",
      "calculate ---> calcul\n",
      "calculator ---> calcul\n",
      "calculated ---> calcul\n",
      "calculating ---> calcul\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "pstemmer = PorterStemmer()\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "toks = ['calculate', 'calculator', 'calculated', 'calculating']\n",
    "\n",
    "# Porter\n",
    "print(\"Porter Stemmer\", \"\\n\")\n",
    "for tok in toks:\n",
    "    print(tok + ' ----> ' + pstemmer.stem(tok))\n",
    "\n",
    "print(\"\\n\", \"Snowball Stemmer\", \"\\n\")\n",
    "# Snowball\n",
    "for tok in toks:\n",
    "    print(tok + ' ---> ' + stemmer.stem(tok))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:15:58.051495Z",
     "start_time": "2023-06-20T22:15:58.035443Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          Resume_str Category  \\\n0           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...       HR   \n1           HR SPECIALIST, US HR OPERATIONS      ...       HR   \n2           HR DIRECTOR       Summary      Over 2...       HR   \n3           HR SPECIALIST       Summary    Dedica...       HR   \n4           HR MANAGER         Skill Highlights  ...       HR   \n\n                                      tokenized_text  \\\n0  [HR, ADMINISTRATOR/MARKETING, ASSOCIATE, HR, A...   \n1  [HR, SPECIALIST, ,, US, HR, OPERATIONS, Summar...   \n2  [HR, DIRECTOR, Summary, Over, 20, years, exper...   \n3  [HR, SPECIALIST, Summary, Dedicated, ,, Driven...   \n4  [HR, MANAGER, Skill, Highlights, HR, SKILLS, H...   \n\n                                       filtered_text  \\\n0  [hr, administrator/marketing, associate, hr, a...   \n1  [hr, specialist, ,, us, hr, operations, summar...   \n2  [hr, director, summary, 20, years, experience,...   \n3  [hr, specialist, summary, dedicated, ,, driven...   \n4  [hr, manager, skill, highlights, hr, skills, h...   \n\n                                     lemmatized_text  \\\n0  [hr, administrator/marketing, associate, hr, a...   \n1  [hr, specialist, ,, u, hr, operation, summary,...   \n2  [hr, director, summary, 20, year, experience, ...   \n3  [hr, specialist, summary, dedicated, ,, driven...   \n4  [hr, manager, skill, highlight, hr, skill, hr,...   \n\n                                      processed_text  \n0  hr administrator/marketing associate hr admini...  \n1  hr specialist , u hr operation summary versati...  \n2  hr director summary 20 year experience recruit...  \n3  hr specialist summary dedicated , driven , dyn...  \n4  hr manager skill highlight hr skill hr departm...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Resume_str</th>\n      <th>Category</th>\n      <th>tokenized_text</th>\n      <th>filtered_text</th>\n      <th>lemmatized_text</th>\n      <th>processed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n      <td>HR</td>\n      <td>[HR, ADMINISTRATOR/MARKETING, ASSOCIATE, HR, A...</td>\n      <td>[hr, administrator/marketing, associate, hr, a...</td>\n      <td>[hr, administrator/marketing, associate, hr, a...</td>\n      <td>hr administrator/marketing associate hr admini...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HR SPECIALIST, US HR OPERATIONS      ...</td>\n      <td>HR</td>\n      <td>[HR, SPECIALIST, ,, US, HR, OPERATIONS, Summar...</td>\n      <td>[hr, specialist, ,, us, hr, operations, summar...</td>\n      <td>[hr, specialist, ,, u, hr, operation, summary,...</td>\n      <td>hr specialist , u hr operation summary versati...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HR DIRECTOR       Summary      Over 2...</td>\n      <td>HR</td>\n      <td>[HR, DIRECTOR, Summary, Over, 20, years, exper...</td>\n      <td>[hr, director, summary, 20, years, experience,...</td>\n      <td>[hr, director, summary, 20, year, experience, ...</td>\n      <td>hr director summary 20 year experience recruit...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HR SPECIALIST       Summary    Dedica...</td>\n      <td>HR</td>\n      <td>[HR, SPECIALIST, Summary, Dedicated, ,, Driven...</td>\n      <td>[hr, specialist, summary, dedicated, ,, driven...</td>\n      <td>[hr, specialist, summary, dedicated, ,, driven...</td>\n      <td>hr specialist summary dedicated , driven , dyn...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HR MANAGER         Skill Highlights  ...</td>\n      <td>HR</td>\n      <td>[HR, MANAGER, Skill, Highlights, HR, SKILLS, H...</td>\n      <td>[hr, manager, skill, highlights, hr, skills, h...</td>\n      <td>[hr, manager, skill, highlight, hr, skill, hr,...</td>\n      <td>hr manager skill highlight hr skill hr departm...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lemmatization\n",
    "\n",
    "# Tokenization\n",
    "resume_data['tokenized_text'] = resume_data['Resume_str'].apply(word_tokenize)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "resume_data['filtered_text'] = resume_data['tokenized_text'].apply(lambda x: [word.lower() for word in x if word.lower() not in stop_words])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "resume_data['lemmatized_text'] = resume_data['filtered_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Convert lemmatized text back to string\n",
    "resume_data['processed_text'] = resume_data['lemmatized_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "resume_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:16:42.547453Z",
     "start_time": "2023-06-20T22:16:26.701082Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text Classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(resume_data['processed_text'], resume_data['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Train a LinearSVC model\n",
    "lsvc = LinearSVC(C=0.1, dual=False, max_iter=1000)\n",
    "lsvc.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train a K-Nearest Neighbour\n",
    "knn = KNeighborsClassifier(n_neighbors=100)\n",
    "knn.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train a Naive Bayes model\n",
    "cnb = ComplementNB(alpha=0.1)\n",
    "cnb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train a Nearest Centroid model\n",
    "nc = NearestCentroid()\n",
    "nc.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train a Ridge Classifier model\n",
    "rc = RidgeClassifier(alpha=1.0, solver=\"sparse_cg\")\n",
    "rc.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "lsvc_accuracy = lsvc.score(X_test_tfidf, y_test)\n",
    "rf_accuracy = rf.score(X_test_tfidf, y_test)\n",
    "knn_accuracy = knn.score(X_test_tfidf, y_test)\n",
    "cnb_accuracy = cnb.score(X_test_tfidf, y_test)\n",
    "nc_accuracy = nc.score(X_test_tfidf, y_test)\n",
    "rc_accuracy = rc.score(X_test_tfidf, y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:17:04.714048Z",
     "start_time": "2023-06-20T22:16:48.486522Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Accuracy: 0.6297786720321932\n",
      "Random Forest Accuracy 0.613682092555332\n",
      "KNN Accuracy 0.5633802816901409\n",
      "Complement Naive Bayes Accuracy 0.5432595573440644\n",
      "Nearest Centroid Accuracy 0.5814889336016097\n",
      "Ridge Classifier Accuracy 0.6599597585513078\n",
      "    dimensionality: 32685\n",
      "    density: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print('Linear SVC Accuracy:', lsvc_accuracy)\n",
    "print('Random Forest Accuracy', rf_accuracy)\n",
    "print('KNN Accuracy', knn_accuracy)\n",
    "print('Complement Naive Bayes Accuracy', cnb_accuracy)\n",
    "print('Nearest Centroid Accuracy', nc_accuracy)\n",
    "print('Ridge Classifier Accuracy', rc_accuracy)\n",
    "print(f\"    dimensionality: {lsvc.coef_.shape[1]}\")\n",
    "print(f\"    density: {density(lsvc.coef_)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:17:08.005845Z",
     "start_time": "2023-06-20T22:17:07.991490Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MaxEnt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "Category\nINFORMATION-TECHNOLOGY    120\nBUSINESS-DEVELOPMENT      120\nACCOUNTANT                118\nADVOCATE                  118\nFINANCE                   118\nENGINEERING               118\nCHEF                      118\nFITNESS                   117\nAVIATION                  117\nSALES                     116\nHEALTHCARE                115\nCONSULTANT                115\nBANKING                   115\nCONSTRUCTION              112\nPUBLIC-RELATIONS          111\nHR                        110\nDESIGNER                  107\nARTS                      103\nTEACHER                   102\nAPPAREL                    97\nDIGITAL-MEDIA              96\nAGRICULTURE                63\nAUTOMOBILE                 36\nBPO                        22\nName: count, dtype: int64"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum Entropy (Logistic Regression)\n",
    "# \"Maximum Entropy (ME) is a generalization of logistic regression for multi-class scenarios\" Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent methods for logistic regression and maximum entropy models. Machine Learning 85(1-2):41-75. https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
    "# Display the Categories for classification\n",
    "resume_data.value_counts(\"Category\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:17:12.738698Z",
     "start_time": "2023-06-20T22:17:12.717880Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy 0.6458752515090543\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model\n",
    "lr=LogisticRegression(penalty='l2', dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', C=5, max_iter=1000, multi_class='multinomial')\n",
    "\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "lr_accuracy = lr.score(X_test_tfidf, y_test)\n",
    "\n",
    "print('Logistic Regression Accuracy', lr_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:17:34.520030Z",
     "start_time": "2023-06-20T22:17:14.920640Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic Parsing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roleset:  stop.01 \n",
      "\n",
      "Predicate: 12:0 \n",
      "\n",
      "Arguments: \n",
      " ((PropbankTreePointer(0, 3), 'ARG1'), (PropbankTreePointer(13, 1), 'ARGM-TMP'), (PropbankTreePointer(15, 1), 'ARGM-CAU')) \n",
      "\n",
      "\n",
      " Tree:  (S\n",
      "  (NP-SBJ\n",
      "    (NP\n",
      "      (NP (NNP South) (NNP Korea) (POS 's))\n",
      "      (JJ economic)\n",
      "      (NN boom))\n",
      "    (, ,)\n",
      "    (SBAR\n",
      "      (WHNP-12 (WDT which))\n",
      "      (S\n",
      "        (NP-SBJ (-NONE- *T*-12))\n",
      "        (VP (VBD began) (PP-TMP (IN in) (NP (CD 1986))))))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (VBD stopped)\n",
      "    (NP-TMP (DT this) (NN year))\n",
      "    (PP-PRP\n",
      "      (IN because)\n",
      "      (IN of)\n",
      "      (NP\n",
      "        (NP (VBN prolonged) (NN labor) (NNS disputes))\n",
      "        (, ,)\n",
      "        (NP (NN trade) (NNS conflicts))\n",
      "        (CC and)\n",
      "        (NP (JJ sluggish) (NNS exports)))))\n",
      "  (. .)) \n",
      "\n",
      "Predicate Select:  (VBD stopped) \n",
      "\n",
      "Arguments: \n",
      "\n",
      "ARG1       (NP-SBJ (NP (NP (NNP South) (NNP Korea) (POS 's)) \n",
      "ARGM-TMP   (NP-TMP (DT this) (NN year))\n",
      "ARGM-CAU   (PP-PRP (IN because) (IN of) (NP (NP (VBN prolonge\n",
      "\n",
      " Frameset: \n",
      "\n",
      "<Element 'roleset' at 0x14017fab0>\n",
      "0 turner\n",
      "1 thing turning\n",
      "m direction, location\n"
     ]
    }
   ],
   "source": [
    "# \"As full semantic parsing is very hard to achieve at this time, NLP researchers turn to some “shallow” semantic parsing techniques. Semantic role labelling (SRL) models and algorithms have been developed to automatically label semantic arguments (or frame elements) associated with the predicate (or semantic frame) of a sentence\" https://comp659r1.athabascau.ca/unit8/section2.php (accessed June 20, 2023)\n",
    "\n",
    "# 1. Identify the predicate\n",
    "# 2. Perform word sense disambiguation\n",
    "# 3. Identify semantic arguments in the sentence\n",
    "\n",
    "from nltk.corpus import propbank\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "# PropBank Corpus provides predicate-argument annotation for the Penn Treebank\n",
    "pb_instances = propbank.instances()\n",
    "inst = pb_instances[193]\n",
    "# Sentence is \"South Korea's economic boom which began in 1986 stopped this year because of labor disputes, trade conflicts and sluggish exports\"\n",
    "\n",
    "print(\"Roleset: \", inst.roleset, \"\\n\")\n",
    "print(\"Predicate:\", inst.predicate, \"\\n\")\n",
    "print(\"Arguments: \\n\", inst.arguments, \"\\n\")\n",
    "\n",
    "tree = inst.tree\n",
    "assert tree == treebank.parsed_sents(inst.fileid)[inst.sentnum]\n",
    "print (\"\\n Tree: \", tree, \"\\n\")\n",
    "\n",
    "# Identify the predicate:\n",
    "print(\"Predicate Select: \", inst.predicate.select(tree), \"\\n\")\n",
    "\n",
    "\n",
    "# Arguments:\n",
    "print(\"Arguments: \\n\")\n",
    "for(argloc, argid) in inst.arguments:\n",
    "    print('%-10s %s' % (argid, argloc.select(tree).pformat(500)[:50]))\n",
    "\n",
    "# Frameset files, which define the argument labels\n",
    "print(\"\\n Frameset: \\n\")\n",
    "expose_01 = propbank.roleset('expose.01')\n",
    "turn_01 = propbank.roleset('turn.01')\n",
    "print(turn_01)\n",
    "for role in turn_01.findall(\"roles/role\"):\n",
    "    print(role.attrib['n'], role.attrib['descr'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T22:17:38.185678Z",
     "start_time": "2023-06-20T22:17:38.045118Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
